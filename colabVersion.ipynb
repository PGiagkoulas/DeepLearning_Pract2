{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.layers import Lambda\n",
    "from keras.layers.merge import add\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Concatenate, ConvLSTM2D, Flatten, MaxPooling1D, Conv1D\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.models import Model\n",
    "import csv\n",
    "# convlstm model\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on \"Upload file\" and upload a zip of the dataset\n",
    "# running this command will unzip it in the current dir\n",
    "!unzip HARDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to save model results to csv files\n",
    "def saveResults(name, fittingProcess, accuracy, aux_accuracy, loss, aux_loss, n):\n",
    "    loss_history = fittingProcess.history['main_output_loss']\n",
    "    acc_history = fittingProcess.history['main_output_acc']\n",
    "    lstm_loss_history = fittingProcess.history['aux_output_loss']\n",
    "    lstm_acc_history = fittingProcess.history['aux_output_acc']\n",
    "    val_loss_history = fittingProcess.history['val_main_output_loss']\n",
    "    val_acc_history = fittingProcess.history['val_main_output_acc']\n",
    "    val_lstm_loss_history = fittingProcess.history['val_aux_output_loss']\n",
    "    val_lstm_acc_history = fittingProcess.history['val_aux_output_acc']\n",
    "\n",
    "    with open(name + str(n) + '.csv', \"w\") as outfile:\n",
    "        outfile.write(\"loss,accuracy,val_loss,val_acc\")\n",
    "        outfile.write(\"\\n\")\n",
    "        for ind in range(len(loss_history)):\n",
    "            outfile.write(\n",
    "                str(loss_history[ind]) + ',' + str(acc_history[ind]) + ',' + str(val_loss_history[ind]) + ',' + str(\n",
    "                    val_acc_history[ind]))\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "    with open(name + '-lstm' + str(n) + '.csv', \"w\") as outfile:\n",
    "        outfile.write(\"lstm_loss,lstm_accuracy,val_lstm_loss,val_lstm_acc\")\n",
    "        outfile.write(\"\\n\")\n",
    "        for ind in range(len(loss_history)):\n",
    "            outfile.write(str(lstm_loss_history[ind]) + ',' + str(lstm_acc_history[ind]) + ',' + str(\n",
    "                val_lstm_loss_history[ind]) + ',' + str(val_lstm_acc_history[ind]))\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "    with open(name + '-modelevaluate' + str(n) + '.csv', \"w\") as outfile:\n",
    "        outfile.write(\"loss,\")\n",
    "        outfile.write(\"accuracy\")\n",
    "        outfile.write(\"aux_loss,\")\n",
    "        outfile.write(\"aux_accuracy\")\n",
    "        outfile.write(\"\\n\")\n",
    "        outfile.write(str(accuracy) + ',')\n",
    "        outfile.write(str(loss) + ',')\n",
    "        outfile.write(str(aux_accuracy))\n",
    "        outfile.write(str(aux_loss))\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "# helper function to assign hyperparameters\n",
    "def unfold_general_hyperparameters(cfg):\n",
    "    verbose = cfg.get('verbose') if ('verbose' in cfg) else 0\n",
    "    epochs = cfg.get('epochs') if ('epochs' in cfg) else 25\n",
    "    batch_size = cfg.get('batch_size') if ('batch_size' in cfg) else 64\n",
    "    activation = cfg.get('activation') if ('activation' in cfg) else 'relu'\n",
    "    # kernel_size_1D = cfg.get('kernel_size_1D') if ('kernel_size_1D' in cfg) else 3\n",
    "    filters = cfg.get('filters') if ('filters' in cfg) else 64\n",
    "    pool_size = cfg.get('pool_size') if ('pool_size' in cfg) else 2\n",
    "    loss = cfg.get('loss') if ('loss' in cfg) else 'categorical_crossentropy'\n",
    "    out_activation = cfg.get('out_activation') if ('out_activation' in cfg) else 'softmax'\n",
    "    optimizer = cfg.get('optimizer') if ('optimizer' in cfg) else 'adam'\n",
    "    dropout_rate = cfg.get('dropout_rate') if ('dropout_rate' in cfg) else 0.5\n",
    "\n",
    "    return verbose, epochs, batch_size, activation, filters, pool_size, loss, out_activation, optimizer, dropout_rate\n",
    "\n",
    "# helper function for PCA feature selection\n",
    "def feature_selection(all_aux_trainX, all_aux_testX):\n",
    "    data = np.concatenate((all_aux_trainX, all_aux_testX), axis=0)\n",
    "    scaler = MinMaxScaler(feature_range=[0, 1])\n",
    "    data_rescaled = scaler.fit_transform(data)\n",
    "    pca = PCA(n_components=175)\n",
    "    dataset = pca.fit_transform(data_rescaled)\n",
    "    aux_trainX = dataset[0:all_aux_trainX.shape[0]][:]\n",
    "    aux_testX = dataset[all_aux_trainX.shape[0]:][:]\n",
    "    return aux_trainX, aux_testX\n",
    "\n",
    "# residual lstm layer generator\n",
    "def residual_lstm_layers(input, rnn_width, rnn_depth, rnn_dropout):\n",
    "    x = input\n",
    "    for i in range(rnn_depth):\n",
    "        return_sequences = i < rnn_depth - 1\n",
    "        # if the return_sequences is true, which means that this LSTM layer will output 3D instead of 2D(By default LSTM output 2D(the last time step of sequence)).\n",
    "        # have the LSTM output a value for each time step in the input data.\n",
    "        x_rnn = LSTM(rnn_width, recurrent_dropout=rnn_dropout, dropout=rnn_dropout, return_sequences=return_sequences)(\n",
    "            x)\n",
    "        if return_sequences:\n",
    "\n",
    "            if i > 0 or input.shape[-1] == rnn_width:\n",
    "                x = add([x, x_rnn])\n",
    "            else:\n",
    "\n",
    "                x = x_rnn\n",
    "        else:\n",
    "            # Last layer does not return sequences, just the last element\n",
    "            # so we select only the last element of the previous output.\n",
    "            def slice_last(x):\n",
    "                return x[..., -1, :]\n",
    "\n",
    "            x = add([Lambda(slice_last)(x), x_rnn])\n",
    "            # x = TimeDistributed(Dense(6, activation='softmax'))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a multi-input/multi-output ConvLSTM model\n",
    "def evaluate_convlstm_multi_model(trainX, trainy, testX, testy, aux_trainX, aux_trainy, aux_testX, aux_testy, cfg, n):\n",
    "    ## datastuff\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    n_aux_features = aux_trainX.shape[1]\n",
    "    # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "    n_steps, n_length = 4, 32\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "\n",
    "    ## parameterstuff\n",
    "    verbose, epochs, batch_size, activation, \\\n",
    "    filters, pool_size, loss, out_activation, \\\n",
    "    optimizer, dropout_rate = unfold_general_hyperparameters(cfg)\n",
    "    kernel_size_2D = cfg.get('kernel_size_2D') if ('kernel_size_2D' in cfg) else (1, 3)\n",
    "\n",
    "    ## modelstuff\n",
    "    main_input = Input(shape=(n_steps, 1, n_length, n_features), dtype='float32', name='main_input')\n",
    "    lstm_out = ConvLSTM2D(filters=filters, kernel_size=kernel_size_2D, activation=activation)(main_input)\n",
    "    x = Dropout(rate=dropout_rate)(lstm_out)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(100, activation=activation)(x)\n",
    "    # auxiliary output of CNNLSTM part\n",
    "    auxiliary_output = Dense(n_outputs, activation=out_activation, name='aux_output')(x)\n",
    "\n",
    "    # flatten output\n",
    "    lstm_out_flat = Flatten()(lstm_out)\n",
    "    auxiliary_input = Input(shape=(n_aux_features,), name='aux_input')\n",
    "    # combine inputs\n",
    "    x = Concatenate()([lstm_out_flat, auxiliary_input])\n",
    "    # rest of the network\n",
    "    x = Dense(128, activation=activation)(x)\n",
    "    x = Dense(64, activation=activation)(x)\n",
    "    x = Dense(32, activation=activation)(x)\n",
    "    # final output\n",
    "    main_output = Dense(n_outputs, activation=out_activation, name='main_output')(x)\n",
    "    model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    ## train/test\n",
    "    history = model.fit(x=[trainX, aux_trainX], y=[trainy, aux_trainy], epochs=epochs, batch_size=batch_size,\n",
    "                        verbose=verbose, validation_data=([testX, aux_testX], [testy, aux_testy]))\n",
    "    # print(history.history)\n",
    "    # print(model.metrics_names)\n",
    "    # print(model.summary())\n",
    "    _, loss, aux_loss, accuracy, aux_acc = model.evaluate(x=[testX, aux_testX], y=[testy, aux_testy],\n",
    "                                                          batch_size=batch_size, verbose=1)\n",
    "    saveResults(\"convLstmMulti\", history, accuracy, aux_acc, loss, aux_loss, n)\n",
    "\n",
    "    return accuracy, aux_acc\n",
    "\n",
    "\n",
    "# fit and evaluate a multi-input/multi-output CNN-LSTM model\n",
    "def evaluate_cnnlstm_multi_model(trainX, trainy, testX, testy, aux_trainX, aux_trainy, aux_testX, aux_testy, cfg, n):\n",
    "    ## data stuff\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    # reshape data into time steps of sub-sequences\n",
    "    n_steps, n_length = 4, 32\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "\n",
    "\n",
    "    ## parameterstuff\n",
    "    verbose, epochs, batch_size, activation,\\\n",
    "    filters, pool_size, loss, out_activation, \\\n",
    "    optimizer, dropout_rate = unfold_general_hyperparameters(cfg)\n",
    "    kernel_size_1D = cfg.get('kernel_size_1D') if ('kernel_size_1D' in cfg) else 3\n",
    "\n",
    "    ## define model\n",
    "    main_input = Input(shape=(None, n_length, n_features), dtype='float32', name='main_input')\n",
    "    x = TimeDistributed(Conv1D(filters=filters, kernel_size=kernel_size_1D, activation='relu'))(main_input)\n",
    "    x = TimeDistributed(Conv1D(filters=filters, kernel_size=kernel_size_1D, activation='relu'))(x)\n",
    "    x = TimeDistributed(Dropout(dropout_rate))(x)\n",
    "    x = TimeDistributed(MaxPooling1D(pool_size=pool_size))(x)\n",
    "    x = TimeDistributed(Flatten())(x)\n",
    "    lstm_out = LSTM(units=100)(x)\n",
    "    x = Dropout(rate=dropout_rate)(lstm_out)\n",
    "    x = Dense(100, activation=activation)(x)\n",
    "    auxiliary_output = Dense(n_outputs, activation=out_activation, name='aux_output')(x)\n",
    "    num_features = aux_trainX.shape[1]\n",
    "    auxiliary_input = Input(shape=(num_features,), name='aux_input')\n",
    "    # combine inputs\n",
    "    x = Concatenate()([lstm_out, auxiliary_input])\n",
    "    # rest of the network\n",
    "    x = Dense(128, activation=activation)(x)\n",
    "    x = Dense(64, activation=activation)(x)\n",
    "    x = Dense(32, activation=activation)(x)\n",
    "    # final output\n",
    "    main_output = Dense(n_outputs, activation=out_activation, name='main_output')(x)\n",
    "    model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x=[trainX, aux_trainX], y=[trainy, aux_trainy], epochs=epochs, batch_size=batch_size,\n",
    "                        verbose=verbose, validation_data=([testX, aux_testX], [testy, aux_testy]))\n",
    "    _, loss, aux_loss, accuracy, aux_acc = model.evaluate(x=[testX, aux_testX], y=[testy, aux_testy],\n",
    "                                                          batch_size=batch_size, verbose=1)\n",
    "\n",
    "    saveResults(\"cnnLstmMulti\", history, accuracy, aux_acc, loss, aux_loss, n)\n",
    "    return accuracy, aux_acc\n",
    "\n",
    "\n",
    "# fit and evaluate a multi-input/multi-output residual LSTM model\n",
    "def evaluate_res_lstm_multi_model(trainX, trainy, testX, testy, aux_trainX, aux_trainy, aux_testX, aux_testy, cfg, n):\n",
    "    verbose, epochs, batch_size = 0, 25, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    # define model\n",
    "    main_input_res = Input(shape=(n_timesteps, n_features), name='residual_lstm_input')\n",
    "    lstm_out = residual_lstm_layers(main_input_res, rnn_width=9, rnn_depth=4, rnn_dropout=0.2)\n",
    "    dense_out=Dense(100, activation='relu')(lstm_out)\n",
    "    auxiliary_output=Dense(n_outputs, activation='softmax', name='aux_output')(dense_out)\n",
    "    num_features = aux_trainX.shape[1]\n",
    "    auxiliary_input = Input(shape=(num_features,), name='aux_input')\n",
    "    # combine inputs\n",
    "    x = Concatenate()([lstm_out, auxiliary_input])\n",
    "    # rest of the network\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    # final output\n",
    "    main_output = Dense(n_outputs, activation='softmax', name='main_output')(x)\n",
    "    model = Model(inputs=[main_input_res, auxiliary_input], outputs=[main_output, auxiliary_output])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x=[trainX, aux_trainX], y=[trainy, aux_trainy], epochs=epochs, batch_size=batch_size,\n",
    "                        verbose=verbose, validation_data=([testX, aux_testX], [testy, aux_testy]))\n",
    "    _, loss, aux_loss, accuracy, aux_acc = model.evaluate(x=[testX, aux_testX], y=[testy, aux_testy],\n",
    "                                                          batch_size=batch_size, verbose=1)\n",
    "    saveResults(\"resLstmMulti\", history, accuracy, aux_acc, loss, aux_loss, n)\n",
    "    return accuracy, aux_acc\n",
    "\n",
    "\n",
    "# fit and evaluate a multi-input/multi-output stacked LSTM model\n",
    "def evaluate_stacked_lstm_multi_model(trainX, trainy, testX, testy, aux_trainX, aux_trainy, aux_testX, aux_testy, cfg, n):\n",
    "    verbose, epochs, batch_size = 0, 25, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    main_input_stacked = Input(shape=(n_timesteps, n_features), name='stacked_lstm_input')\n",
    "    lstm_out0=LSTM(9, activation='tanh',recurrent_dropout=0.2, dropout=0.2, return_sequences=True)(main_input_stacked)\n",
    "    lstm_out1=LSTM(9, activation='tanh',recurrent_dropout=0.2, dropout=0.2, return_sequences=True)(lstm_out0)\n",
    "    lstm_out2=LSTM(9, activation='tanh',recurrent_dropout=0.2, dropout=0.2, return_sequences=True)(lstm_out1)\n",
    "    lstm_out3=LSTM(9, activation='tanh',recurrent_dropout=0.2, dropout=0.2, return_sequences=False)(lstm_out2)\n",
    "    #lstm_out4=LSTM(9, activation='tanh',recurrent_dropout=0.2, dropout=0.2, return_sequences=False)(lstm_out3)\n",
    "    Dense_out = Dense(100, activation='relu')(lstm_out3)\n",
    "    auxiliary_output = Dense(n_outputs, activation='softmax', name='aux_output')(Dense_out)\n",
    "    num_features = aux_trainX.shape[1]\n",
    "    auxiliary_input = Input(shape=(num_features,), name='aux_input')\n",
    "    # combine inputs\n",
    "    x = Concatenate()([lstm_out3, auxiliary_input])\n",
    "    # rest of the network\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    # final output\n",
    "    main_output = Dense(n_outputs, activation='softmax', name='main_output')(x)\n",
    "    model = Model(inputs=[main_input_stacked, auxiliary_input], outputs=[main_output, auxiliary_output])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x=[trainX, aux_trainX], y=[trainy, aux_trainy], epochs=epochs, batch_size=batch_size,\n",
    "                        verbose=verbose, validation_data=([testX, aux_testX], [testy, aux_testy]))\n",
    "    _, loss, aux_loss, accuracy, aux_acc = model.evaluate(x=[testX, aux_testX], y=[testy, aux_testy],\n",
    "                                                          batch_size=batch_size, verbose=1)\n",
    "    saveResults(\"stackedLstmMulti\", history, accuracy, aux_acc, loss, aux_loss, n)\n",
    "    return accuracy, aux_acc\n",
    "\n",
    "\n",
    "# fit and evaluate ConvLSTM a model\n",
    "def evaluate_convlst_model(trainX, trainy, testX, testy, cfg):\n",
    "    # define model\n",
    "    verbose, epochs, batch_size = 0, 25, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "    n_steps, n_length = 4, 32\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
    "    # trainX = trainX\n",
    "    # trainy = trainy\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        ConvLSTM2D(filters=64, kernel_size=(1, 3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# fit and evaluate a CNN-LSTM model\n",
    "def evaluate_cnnlstm_model(trainX, trainy, testX, testy, cfg):\n",
    "    # define model\n",
    "    verbose, epochs, batch_size = 0, 25, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    # reshape data into time steps of sub-sequences\n",
    "    n_steps, n_length = 4, 32\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None, n_length, n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# fit and evaluate a residual LSTM model\n",
    "def evaluate_res_lstm_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 0, 25, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    input_res = Input(shape=(n_timesteps, n_features), name='residual_lstm_input')\n",
    "    lstm_out = residual_lstm_layers(input_res, rnn_width=9, rnn_depth=8, rnn_dropout=0.2)\n",
    "    Dense_out=Dense(100, activation='relu')(lstm_out)\n",
    "    output=Dense(n_outputs,activation='softmax')(Dense_out)\n",
    "    model = Model(inputs=input_res, outputs=output)\n",
    "    # model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    # fit the network\n",
    "    model.fit(x=trainX, y=trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    _, accuracy = model.evaluate(x=testX, y=testy, batch_size=batch_size, verbose=verbose)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# fit and evaluate a stacked LSTM model\n",
    "def evaluate_stacked_lstm_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 0, 25, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    input_stacked = Input(shape=(n_timesteps, n_features), name='stacked_lstm_input')\n",
    "    lstm_out0=LSTM(9, activation='tanh',recurrent_dropout=0.2, dropout=0.2, return_sequences=True)(input_stacked)\n",
    "    lstm_out1=LSTM(9, activation='tanh',recurrent_dropout=0.2, dropout=0.2, return_sequences=True)(lstm_out0)\n",
    "    lstm_out2=LSTM(9, activation='tanh',recurrent_dropout=0.2, dropout=0.2, return_sequences=True)(lstm_out1)\n",
    "    lstm_out3=LSTM(9, activation='tanh',recurrent_dropout=0.2, dropout=0.2, return_sequences=True)(lstm_out2)\n",
    "    lstm_out4=LSTM(9, activation='tanh',recurrent_dropout=0.2, dropout=0.2, return_sequences=False)(lstm_out3)\n",
    "    Dense_out = Dense(100, activation='relu')(lstm_out4)\n",
    "    output = Dense(n_outputs, activation='softmax')(Dense_out)\n",
    "    model = Model(inputs=input_stacked, outputs=output)\n",
    "    # model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    # fit the network\n",
    "    model.fit(x=trainX, y=trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    _, accuracy = model.evaluate(x=testX, y=testy, batch_size=batch_size, verbose=verbose)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# how to use:\n",
    "\n",
    "# in run experiment: set grid to true or not, set amount of repeats (cross validation)\n",
    "# in run experiment: set the evaluate function, choose from the allModels file\n",
    "# in giveParameters: set values for the hyper-parameters to be tested\n",
    "\n",
    "# the current implementation will poop out (value of repeat) amount of csv's with results during each epoch,\n",
    "# it will override these files per configuration.\n",
    "\n",
    "# if grid is enabled it will print the best configuration at the end, use this anew with only those parameters in giveParameters to get the csv's of the best config\n",
    "\n",
    "def giveParameters():\n",
    "    # learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "    verbose = [1]\n",
    "    batch_size = [64, 128]\n",
    "    optimizer = ['adam', 'sgd']\n",
    "    epochs = [50]\n",
    "    activation = ['relu', 'sigmoid', 'tanh']\n",
    "    kernel_size_2D = [(1, 3)]\n",
    "    kernel_size_1D = [3]\n",
    "    filters = [64, 128]\n",
    "    pool_size = [2]\n",
    "    loss = ['categorical_crossentropy']\n",
    "    out_activation = ['softmax']\n",
    "    dropout_rate = [0, 0.25, 0.5]\n",
    "    return dict(verbose=verbose, epochs=epochs, batch_size=batch_size, activation=activation,\n",
    "                kernel_size_2D=kernel_size_2D, kernel_size_1D=kernel_size_1D, filters=filters, pool_size=pool_size,\n",
    "                loss=loss, out_activation=out_activation, optimizer=optimizer, dropout_rate=dropout_rate)\n",
    "\n",
    "\n",
    "# run an experiment\n",
    "def run_experiment(repeats=10):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy, aux_trainX, aux_trainy, aux_testX, aux_testy = load_dataset()\n",
    "    grid = True\n",
    "\n",
    "    cfg_list = defineConfigurations()  # list with all possible configurations\n",
    "\n",
    "    gridresults = list()\n",
    "\n",
    "    for cfg in cfg_list:\n",
    "        scores = list()\n",
    "        for r in range(repeats):\n",
    "            score, aux_score = evaluate_stacked_lstm_multi_model(trainX, trainy, testX, testy, aux_trainX, aux_trainy,\n",
    "                                                            aux_testX, aux_testy, cfg,\n",
    "                                                            r)  # change if you want to run another model\n",
    "            score = score * 100.0\n",
    "            aux_score = aux_score * 100.00  # also remove everything regarding aux_score if a different model is used\n",
    "            print('>#%d: LSTM = %.3f and Multi = %.3f' % (r + 1, score, aux_score))\n",
    "            scores.append(score)\n",
    "        gridresults.append((cfg, scores))\n",
    "\n",
    "    if grid:\n",
    "        summarize_gridresults(gridresults)\n",
    "    printBestGrid(gridresults)\n",
    "\n",
    "\n",
    "########################### grid functions\n",
    "\n",
    "def summarize_gridresults(gridresults):\n",
    "    for x in gridresults:\n",
    "        print(x[0])  # prints the config\n",
    "        summarize_results(x[1])  # prints the scores\n",
    "\n",
    "\n",
    "def defineConfigurations():\n",
    "    cfgs = list()\n",
    "    parameters = giveParameters()\n",
    "    return list((dict(zip(parameters, x)) for x in product(*parameters.values())))\n",
    "\n",
    "\n",
    "def printBestGrid(gridresults):\n",
    "    best = gridresults[0]\n",
    "    for x in gridresults:\n",
    "        if mean(x[1]) > mean(best[1]):\n",
    "            best = x\n",
    "    print(\"best result:\")\n",
    "    print(best[0])\n",
    "    summarize_results(best[1])\n",
    "\n",
    "\n",
    "########################### preparing the data\n",
    "\n",
    "\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values\n",
    "\n",
    "\n",
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(filenames, prefix=''):\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = dstack(loaded)\n",
    "    return loaded\n",
    "\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_timeseries_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_' + group + '.txt', 'total_acc_y_' + group + '.txt', 'total_acc_z_' + group + '.txt']\n",
    "    # body acceleration\n",
    "    filenames += ['body_acc_x_' + group + '.txt', 'body_acc_y_' + group + '.txt', 'body_acc_z_' + group + '.txt']\n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_' + group + '.txt', 'body_gyro_y_' + group + '.txt', 'body_gyro_z_' + group + '.txt']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_' + group + '.txt')\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def load_const_dataset_group(group, prefix=''):\n",
    "    X = load_file(prefix + group + '/X_' + group + '.txt')\n",
    "    y = load_file(prefix + group + '/y_' + group + '.txt')\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_timeseries_dataset_group('train', prefix + 'HARDataset/')\n",
    "    all_aux_trainX, aux_trainy = load_const_dataset_group('train', prefix + 'HARDataset/')\n",
    "    print(\">> Time series data shape: {0} , {1}\".format(trainX.shape, trainy.shape))\n",
    "    print(\">> Constant data shape: {0} , {1}\".format(all_aux_trainX.shape, aux_trainy.shape))\n",
    "    # load all test\n",
    "    testX, testy = load_timeseries_dataset_group('test', prefix + 'HARDataset/')\n",
    "    all_aux_testX, aux_testy = load_const_dataset_group('test', prefix + 'HARDataset/')\n",
    "    print(\">> Time series data shape: {0} , {1}\".format(testX.shape, testy.shape))\n",
    "    print(\">> Constant data shape: {0} , {1}\".format(all_aux_testX.shape, aux_testy.shape))\n",
    "    # feature selection on constant features\n",
    "    aux_trainX, aux_testX = feature_selection(all_aux_trainX, all_aux_testX)\n",
    "    # zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "    aux_trainy = aux_trainy - 1\n",
    "    aux_testy = aux_testy - 1\n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    aux_trainy = to_categorical(aux_trainy)\n",
    "    aux_testy = to_categorical(aux_testy)\n",
    "    print(\">> Final shapes of time series dataset: {0}, {1}, {2}, {3}\".format(trainX.shape, trainy.shape, testX.shape,\n",
    "                                                                              testy.shape))\n",
    "    print(\">> Final shapes of constant dataset: {0}, {1}, {2}, {3}\".format(aux_trainX.shape, aux_trainy.shape,\n",
    "                                                                           aux_testX.shape, aux_testy.shape))\n",
    "    return trainX, trainy, testX, testy, aux_trainX, aux_trainy, aux_testX, aux_testy\n",
    "\n",
    "\n",
    "########################### summarize scores\n",
    "\n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### run the experiment\n",
    "\n",
    "run_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
